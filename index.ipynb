{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **概要**\n",
    "\n",
    "AlphaZeroのアルゴリズムは以下のような手順で構成される：\n",
    "\n",
    "1. 自己対戦による学習: AlphaZeroは、自分自身と対戦しながら学習する。ランダムな手法からスタートし、強化学習アルゴリズムを用いて次第に強くなる。\n",
    "\n",
    "2. モンテカルロ木探索: AlphaZeroは、モンテカルロ木探索（MCTS）を使用してゲームのツリーを探索する。これにより、最適な手を見つけるために局面を評価し、次の手を決定する。\n",
    "\n",
    "3. ニューラルネットワーク: AlphaZeroは、ゲームの状態を評価するためのニューラルネットワークを使用する。このニューラルネットワークは、ゲームの局面を入力とし、局面の価値を出力する。価値は、勝率や局面の良さなどを示す。\n",
    "\n",
    "4. 強化学習: AlphaZeroは、報酬を最大化するようにニューラルネットワークを調整することで学習します。報酬は、ゲームの勝利や敗北などによって与えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ライブラリ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# pytorchのライブラリをインポート\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "# 対戦状況を可視化するためのライブラリをインポート\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "trans = torchvision.transforms.ToTensor()  # データをpytorchのテンソルに変換する関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **オセロのルールを反映させたクラス**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オセロの状態を表すクラス(構造体)\n",
    "# 8x8の盤面を持ち、各マスには黒か白か空かのいずれかが入る\n",
    "# 盤面の状態を配列で表現する\n",
    "class Board():\n",
    "    # 8x8の配列を初期化する\n",
    "    def __init__(self):\n",
    "        self.board = [[0 for i in range(8)] for j in range(8)]\n",
    "        self.board[3][3] = 1    # 白(◯)\n",
    "        self.board[4][4] = 1    # 白\n",
    "        self.board[3][4] = -1    # 黒(×)\n",
    "        self.board[4][3] = -1    # 黒\n",
    "        self.player = -1         # 黒が先手\n",
    "    # 盤面の配列を取得する\n",
    "    def getBoard(self):\n",
    "        return self.board\n",
    "    # 盤面の状態を表示する\n",
    "    def show(self):\n",
    "        for row in self.board:\n",
    "            print(\"|\", end=\"\")\n",
    "            for cell in row:\n",
    "                if cell == 0:\n",
    "                    print(\" \", end=\"\")  # end=\"\" で改行しない\n",
    "                elif cell == 1:\n",
    "                    print(\"o\", end=\"\")\n",
    "                elif cell == -1:\n",
    "                    print(\"x\", end=\"\")\n",
    "            print(\"|\")                     # 一行表示して改行\n",
    "        print(\"player: o\") if self.player == 1 else print(\"player: x\")\n",
    "    # ファイルに盤面の状態を書き込む\n",
    "    def fshow(self, filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            for row in self.board:\n",
    "                f.write(\"|\")\n",
    "                for cell in row:\n",
    "                    if cell == 0:\n",
    "                        f.write(\" \")\n",
    "                    elif cell == 1:\n",
    "                        f.write(\"o\")\n",
    "                    elif cell == -1:\n",
    "                        f.write(\"x\")\n",
    "                    f.write(str(cell))\n",
    "                f.write(\"|\\n\")\n",
    "            if self.player == 1:\n",
    "                f.write(\"player: o\")\n",
    "            else:\n",
    "                f.write(\"player: x\")\n",
    "    # 置く場所が盤面上にあるか判定する\n",
    "    def outofBoard(self, x, y):\n",
    "        return x < 0 or x >= 8 or y < 0 or y >= 8\n",
    "    # (x, y)に石を置くとき、(dx, dy)方向にひっくり返す石があるか判定する\n",
    "    def canPutDir(self, x, y, dx, dy):\n",
    "        i = 1\n",
    "        if self.outofBoard(x + dx, y + dy):\n",
    "            return False\n",
    "        else:\n",
    "            while self.board[y + dy * i][x + dx * i] == -(self.player):    # 相手の石が続く限り\n",
    "                i += 1\n",
    "                if self.outofBoard(x + dx * i, y + dy * i):                 # 盤面外に出たら\n",
    "                    return False\n",
    "                elif self.board[y + dy * i][x + dx * i] == 0:               # 石がない場所があれば\n",
    "                    return False\n",
    "                elif self.board[y + dy * i][x + dx * i] == self.player:     # 自分の石で挟めるなら\n",
    "                    return True\n",
    "    # (x, y)に石を置けるか判定する\n",
    "    def canPut(self, x, y):\n",
    "        if not self.outofBoard(x,y) and self.board[y][x] != 0:                                           # すでに石が置かれている\n",
    "            return False\n",
    "        # 周囲8方向を調べる; 1方向でもひっくり返せるなら置ける\n",
    "        for dx in range(-1, 2):\n",
    "            for dy in range(-1, 2):\n",
    "                if dx == 0 and dy == 0: \n",
    "                    continue\n",
    "                if self.canPutDir(x, y, dx, dy):                             # (dx, dy)方向にひっくり返せる石がある\n",
    "                    return True\n",
    "        return False\n",
    "    # (x, y)に石を置いたときの盤面クラスを返す\n",
    "    def put(self, x, y):\n",
    "        newBoard = Board()\n",
    "        newBoard.board = [row[:] for row in self.board] # 盤面をコピー\n",
    "        newBoard.player = -self.player   # プレイヤーを交代\n",
    "        newBoard.board[y][x] = self.player  # 石を置く\n",
    "        # ひっくり返せる石を全てひっくり返す\n",
    "        for dx in range(-1, 2):\n",
    "            for dy in range(-1, 2):\n",
    "                if dx == 0 and dy == 0:\n",
    "                    continue\n",
    "                if self.canPutDir(x, y, dx, dy):\n",
    "                    i = 1\n",
    "                    while newBoard.board[y + dy * i][x + dx * i] == -self.player:\n",
    "                        newBoard.board[y + dy * i][x + dx * i] = self.player    # 石をひっくり返す\n",
    "                        i += 1\n",
    "        return newBoard\n",
    "    # 石を置ける場所があるか判定する\n",
    "    def canPlay(self):\n",
    "        for y in range(8):\n",
    "            for x in range(8):\n",
    "                if self.canPut(x, y):\n",
    "                    return True\n",
    "        return False\n",
    "    # 石を置ける場所と置いた場合の盤面クラスを辞書型で返す\n",
    "    def choices(self):          # 動的計画法において、for choice in board.choices(): で呼び出す\n",
    "        choices = {}\n",
    "        for y in range(8):\n",
    "            for x in range(8):\n",
    "                if self.canPut(x, y):\n",
    "                    choices[(x, y)] = self.put(x, y)\n",
    "        return choices\n",
    "    # 石を置ける場所がなく場合にプレイヤーを交代するか判定し、交代する (交代する場合は新たな盤面を返す)\n",
    "    def passPlayer(self):   # Board.passPlayer()で呼び出す\n",
    "        if not self.canPlay():\n",
    "            # 新たな盤面を作成し、プレイヤーを交代する\n",
    "            newBoard = Board()\n",
    "            newBoard.board = [row[:] for row in self.board] # 盤面をコピー\n",
    "            newBoard.player = -self.player\n",
    "            return newBoard\n",
    "        else:\n",
    "            return self\n",
    "    # 盤面が等しいか判定する\n",
    "    def isEq(self, other):\n",
    "        return self.board == other.board and self.player == other.player\n",
    "    # 勝負がついたか判定する\n",
    "    def isEnd(self):\n",
    "        if not self.canPlay() and not self.passPlayer().canPlay():\n",
    "            # print(\"Game Over\")\n",
    "            # self.winner()\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    # 石の数を数え、勝敗を判定する\n",
    "    def counter(self, num):\n",
    "        tot = 0\n",
    "        for row in self.board:\n",
    "            for cell in row:\n",
    "                if cell == num:\n",
    "                    tot += 1\n",
    "        return tot\n",
    "    def winner(self):\n",
    "        if not self.isEnd():\n",
    "            return\n",
    "        print(\"Game Over\")\n",
    "        black = self.counter(-1)\n",
    "        white = self.counter(1)\n",
    "        if black > white:\n",
    "            print(\"Black(×) wins!\")\n",
    "            return 1\n",
    "        elif black < white:\n",
    "            print(\"White(◯) wins!\")\n",
    "            return -1\n",
    "        else:\n",
    "            print(\"Draw\")\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **実際に人間同士でプレイしてみる**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ゲームをプレイする\n",
    "board = Board()\n",
    "while not board.isEnd():\n",
    "    time.sleep(1.0)                                     # 0.5秒待つ\n",
    "    clear_output()                                      # 画面をクリア\n",
    "    board.show()                                        # 盤面を表示\n",
    "    if board.canPlay():\n",
    "        print(\"Put coordinate x, y: \", end=\"\")\n",
    "        x, y = map(int, input().split())                # 入力を受け取る\n",
    "        print(\"(\", x, \", \", y,\")\")                      # 入力を表示\n",
    "        if board.canPut(x, y):\n",
    "            board = board.put(x, y)\n",
    "        else:\n",
    "            print(\"Can't put\")\n",
    "    else:\n",
    "        board = board.passPlayer()                      # 石を置けない場合はプレイヤーを交代\n",
    "time.sleep(1.0)\n",
    "clear_output()\n",
    "board.show()\n",
    "board.winner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **探索アルゴリズム**\n",
    "\n",
    "**Monte-Carlo-Tree-Search(MCTS)**による学習アルゴリズムを実装する。\n",
    "\n",
    "### **MCTSのアルゴリズム**\n",
    "\n",
    "参考文献：[Monte Carlo Tree Search - Wikipedia](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)\n",
    "\n",
    "モンテカルロ木探索は4つのステップからなる。\n",
    "\n",
    "Nodeは、ゲームの状態を表し、プレイアウトの回数と勝利の回数を保持している。\n",
    "\n",
    "1. 選択：Root Node から始めて、Leaf Node にたどり着くまで、子ノードを選択し続ける。Root Node が現在のゲームの状態で、Leaf Node はシミュレーションが行われていないノード。より有望な方向に木が展開していくように、子ノードの選択を片寄らせる方法を採用するため、以下の Alpha Zero 指標を用いる。\n",
    "2. 展開：Leaf Node が勝負を決するノードでない限り、Leaf Node から有効手の子ノードの中から Child Node を1つ選ぶ。\n",
    "3. シミュレーション：Child Node からプレイアウトを行う。Alpha Zero 指標に基づいてプレイアウトを実行する。\n",
    "4. バックプロパゲーション：Child Node から Root Node へのパスに沿って、プレイアウトの結果を伝搬する。\n",
    "\n",
    "### **Alpha Zero 指標**\n",
    "\n",
    "Alpha Zero 指標は、探索と活用のバランスを取る指標である。\n",
    "\n",
    "$ Q(s, a) + U(s, a) $\n",
    "\n",
    "が最大となる行動を選択する。ここで、$Q(s, a)$ は状態 $s$ で行動 $a$ を取った時の行動価値関数 $(=V(s'))$、$U(s, a)$ は探索項である。\n",
    "\n",
    "$ U(s, a) = c(s) P(s, a) \\frac{\\sqrt{\\sum_{b} N(s, b)}}{1 + N(s, a)} $\n",
    "\n",
    "ここで、$c(s)$ は下記の式で定まる数で、$P(s, a)$ は方策ネットワークによる行動 $a$ の選択確率、$N(s, a)$ は状態 $s$ で行動 $a$ を選択した回数、$\\sum_{b} N(s, b)$ は状態 $s$ に到達した回数である。\n",
    "\n",
    "$ c(s) = c_{init} + \\log{\\frac{1 + N(s) + c_{base}}{c_{base}}} $\n",
    "\n",
    "ここで、$c_{base}$ はハイパーパラメータであり、$N(s)$ は状態 $s$ に到達した回数である。\n",
    "\n",
    "### **Q関数の更新**\n",
    "\n",
    "チェスは、遷移確率が one-hot つまり $P(s'|s,a)=1$ であるため、状態価値関数 $V(s)$ と行動価値関数 $Q(s, a)$ は同じものとして扱うことができる。したがって、\n",
    "盤面の状態を64次元のベクトル(とる値は ${-1,0,1}$ )で表現し、それに対して有利不利を $[-1,1]$ の範囲で出力する状態行動価値関数 $Q(s,a)=V(s')$ を学習する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MCT Node クラスの実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte-Carlo Tree のノードを表すクラス\n",
    "class MCTSNode:\n",
    "    def __init__(self, board):\n",
    "        self.board = board          # Boardクラス\n",
    "        self.parent = None          # 親ノード\n",
    "        self.children = []          # 子ノード\n",
    "        self.visits = 0             # ノードを訪れた回数\n",
    "        self.score = 0              # ノードの評価値\n",
    "    # ノードの展開(子ノードを追加)\n",
    "    def expand(self):\n",
    "        if self.board.isEnd():          # ゲーム終了時は展開しない\n",
    "            return\n",
    "        elif len(self.children) > 0:    # 既に展開済みの場合は展開しない\n",
    "            return\n",
    "        else:\n",
    "            choices = self.board.choices()              # 石を置ける場所と置いた場合の盤面クラスを取得(Boardクラスのchoicesメソッド)\n",
    "            for choice, board in choices.items():       # 石を置ける場所全てについて、子ノードに追加\n",
    "                node = MCTSNode(board)\n",
    "                node.parent = self\n",
    "                self.children.append(node)\n",
    "    # 子ノードの選択確率を計算し、石を置いた場所にその確率を当てはめた二次元配列を返す\n",
    "    def probabilities(self):\n",
    "        probs = [[0 for i in range(8)] for j in range(8)]\n",
    "        for child in self.children:\n",
    "            x, y = child.board.choices()    # 石を置いた場所を取得\n",
    "            probs[y][x] = child.visits/self.visits    # 確率を計算\n",
    "        # 確率の合計がほぼ1であることを確認し、あまりにも差がある場合には、エラーを出力\n",
    "        if abs(sum([sum(prob) for prob in probs]) - 1) > 0.01:\n",
    "            print(\"Error: sum(prob) != 1\")\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mctsNodeで、choiceを選択することの評価値(行動価値関数と探索項の和)を返す\n",
    "def alpha_zero_score(mctsNode, key, mcTree, policy, Q_value):\n",
    "    # ハイパーパラメータ\n",
    "    c_base = 1000\n",
    "    c_init = 1.25\n",
    "    # 行動価値関数の値を計算\n",
    "    value = Q_value.predict(mctsNode.board, key)\n",
    "    # ノードの探索項を計算\n",
    "    N = mctsNode.visits                                 # mctsNodeを訪れた回数\n",
    "    childNode = mcTree.move_cop(key)                    # keyの手を打ったときのノードを取得\n",
    "    Nc = childNode.visits                               # mctsNodeからchoiceを選択した回数\n",
    "    C = c_init + np.log((N + c_base + 1) / c_base)      # ノードの訪問回数で決まる係数\n",
    "    exploration = C * np.sqrt(N) / (1 + Nc) * policy.predict(mctsNode.board)[key[0],key[1]]          # 探索項\n",
    "    # ノードの評価値と探索項の和を返す\n",
    "    return value + exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC Tree クラスの実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte-Carlo Tree のクラス\n",
    "class MCTree:\n",
    "    def __init__(self, board):\n",
    "        self.root = MCTSNode(board)                 # ルートノードを作成\n",
    "        self.current = self.root                    # 現在のノード\n",
    "    # ノードの選択\n",
    "    def select(self, Policy, Q_value):\n",
    "        node = self.current\n",
    "        choices = self.current.board.choices()              # 石を置ける場所と置いた場合の盤面を取得\n",
    "        # alpha_zero_score()を用いて、評価値が最大の子ノードを選択\n",
    "        max_score = -math.inf\n",
    "        for choice, board in choices.items():\n",
    "            # time.sleep(1.0)    # 0.5秒待つ\n",
    "            # clear_output()    # 画面をクリア\n",
    "            # print(\"choice: player is\", board.player)    # デバグ用\n",
    "            # board.show()    # デバグ用\n",
    "            for child in node.children:\n",
    "                # time.sleep(1.0)    # 0.5秒待つ\n",
    "                # clear_output()    # 画面をクリア\n",
    "                # print(\"child: player is\", child.board.player)    # デバグ用\n",
    "                # child.board.show()    # デバグ用\n",
    "                if child.board.isEq(board):\n",
    "                    # print(\"I found child node\\n\")    # デバグ用\n",
    "                    score = alpha_zero_score(node, choice, self, Policy, Q_value)\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        node = child\n",
    "        # time.sleep(1.0)    # 0.5秒待つ\n",
    "        # clear_output()    # 画面をクリア\n",
    "        # print(\"I choose: \")    # デバグ用\n",
    "        # node.board.show()    # デバグ用\n",
    "        return node\n",
    "    # ノードの移動(クラスをコピーせず、現在のノードを変更する)\n",
    "    def move_cur(self, key):                        # key: 石を置く場所を指定\n",
    "        # 現在のノードに対して、keyに石を置いた場合の盤面を辞書のキーから検索\n",
    "        choices = self.current.board.choices()              # 石を置ける場所と置いた場合の盤面を取得\n",
    "        for choice, board in choices.items():\n",
    "            if choice == key:                               # keyに石を置いた場合の盤面クラスを子ノードから見つけたら\n",
    "                for child in self.current.children:\n",
    "                    if child.board.isEq(board):                # 盤面クラスの一致を確認\n",
    "                        self.current = child\n",
    "                        return\n",
    "    # ノードの移動(クラスをコピーして、現在のノードを変更する)\n",
    "    def move_cop(self, key):                       # key: 石を置く場所を指定\n",
    "        # 現在のノードに対して、keyに石を置いた場合の盤面を辞書のキーから検索\n",
    "        choices = self.current.board.choices()              # 石を置ける場所と置いた場合の盤面を取得\n",
    "        for choice, board in choices.items():\n",
    "            if choice == key:                               # keyに石を置いた場合の盤面クラスを子ノードから見つけたら\n",
    "                for child in self.current.children:\n",
    "                    if child.board.isEq(board):                # 盤面クラスの一致を確認\n",
    "                        self.current = child\n",
    "                        return child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **自己対戦クラスの実装**\n",
    "\n",
    "プレイヤークラスのメソッドは以下の通りです。\n",
    "\n",
    "#### **MC Tree の初期化**\n",
    "\n",
    "#### **経験の獲得**\n",
    "\n",
    "今回は、行動毎に報酬を得るわけではなく、１ゲームをプレーした後に報酬を得ることに注意が必要である。さらには、バッチサイズ分ゲームをプレーして、それに対する\n",
    "\n",
    "#### **MC Tree の更新**\n",
    "\n",
    "実際のプレイヤーの行動とそれに対する報酬(勝利:1, 敗北:-1, 引き分け:0)を受け取り、MC Tree に追加します。\n",
    "\n",
    "#### **状態行動価値関数の初期化**\n",
    "\n",
    "状態行動価値関数 $Q_{\\theta}(s_t, a_t)$ について、初期化する。\n",
    "\n",
    "#### **方策の更新**\n",
    "\n",
    "基本的には、マルチステップ方策評価・マルチステップ方策更新を行う。その際には、Greedy Policy とするか、$\\epsilon$-Greedy Policy 、あるいは Softmax Policy とするかを指定する。\n",
    "\n",
    "#### **行動の選択**\n",
    "自分のターンが来たら、方策に従って行動を選択する。決定論的な方策を取るか、確率的な方策を取るかは、方策の種類次第になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, Policy, Q_value):\n",
    "        self.Policy = Policy            # 方策NN\n",
    "        self.Q_value = Q_value          # 行動価値NN            \n",
    "        self.path = [[]]                # プレイしたノード(MCTSNodeクラス)の履歴\n",
    "        self.nodes = []                 # ミニバッチ全体で一度でもプレイしたノード(MCTSNodeクラス)のリスト\n",
    "        self.cursor = 0                 # ミニバッチにおける現在のインデックス\n",
    "        self.reward = [0,0]             # [黒の報酬、白の報酬](勝利: 1, 引き分け: 0.5, 敗北: 0)\n",
    "    # 1手プレイする\n",
    "    def oneplay(self, mctree):\n",
    "        # モンテカルロ木探索\n",
    "        if len(mctree.current.children)==0:             # 子ノードがない場合は展開\n",
    "            mctree.current.expand()\n",
    "            if len(mctree.current.children)==0:         # 打てる手がない場合はパス\n",
    "                mctree.current.board = mctree.current.board.passPlayer()\n",
    "                return mctree\n",
    "        ans = mctree.select(self.Policy, self.Q_value)  # ノードの選択\n",
    "        # time.sleep(1.0)                                 # 0.5秒待つ\n",
    "        # clear_output()                                  # 画面をクリア\n",
    "        # print(\"selected:\")\n",
    "        # ans.board.show()                                # 盤面を表示(デバッグ用)\n",
    "        mctree.current = ans    # ノードの選択と現ノードの変更\n",
    "        # time.sleep(0.2)                                     # 0.5秒待つ\n",
    "        # clear_output()                                      # 画面をクリア\n",
    "        # mctree.current.board.show()                                 # 盤面を表示(デバッグ用)\n",
    "        self.path[self.cursor].append(mctree.current)                # 選択したノードを記録\n",
    "        # self.nodesに進んだ先のノードがなければ追加\n",
    "        if mctree.current not in self.nodes:\n",
    "            self.nodes.append(mctree.current)\n",
    "        return mctree\n",
    "    # 1ゲーム自己対戦する\n",
    "    def play1game(self, mctree):\n",
    "        board = Board()\n",
    "        while not mctree.current.board.isEnd():\n",
    "            mctree = self.oneplay(mctree)\n",
    "            # print(\"p\",end=\"\")\n",
    "        board = mctree.current.board\n",
    "        if board.winner() == 1:\n",
    "            self.reward[0] += 1\n",
    "        elif board.winner() == -1:\n",
    "            self.reward[1] += 1\n",
    "        else:\n",
    "            self.reward[0] += 0.5\n",
    "            self.reward[1] += 0.5\n",
    "    # プレイしたノードの訪問回数と評価値を更新する\n",
    "    def backup(self):\n",
    "        for node in self.path[self.cursor]:                  # プレイしたノード全てについて\n",
    "            node.visits += 1\n",
    "            if node.board.player == 1:          # そのノードでプレイしたのが黒の場合\n",
    "                node.score += self.reward[0]\n",
    "            else:                               # そのノードでプレイしたのが白の場合\n",
    "                node.score += self.reward[1]\n",
    "            # print(\"b\",end=\"\")\n",
    "    # モンテカルロ・シミュレーションのサンプルサイズ分、1ゲーム自己対戦・backupする\n",
    "    def play1batch(self, mctree, sample_size):\n",
    "        for i in range(sample_size):\n",
    "            if i % 10 == 0:\n",
    "                print(\"game\", i)\n",
    "            self.play1game(mctree)\n",
    "            self.backup()\n",
    "            if i != sample_size - 1:\n",
    "                self.cursor += 1\n",
    "                self.path.append([])\n",
    "    # self.nodesを、visit回数の降順にソートする\n",
    "    def sort_nodes(self):\n",
    "        self.nodes.sort(key=lambda x: x.visits, reverse=True)\n",
    "    # データをロードする(盤面、手の選択確率、報酬)\n",
    "    def prepare(self, batch_size):\n",
    "        # ミニバッチのデータを作成\n",
    "        xs = []  # 盤面の状態(入力)\n",
    "        ys = []  # 方策(出力)\n",
    "        zs = []  # 報酬\n",
    "        # player.nodesをミニバッチに分割\n",
    "        cursor = len(self.nodes)\n",
    "        node_groups = []\n",
    "        while cursor > batch_size:\n",
    "            cursor -= batch_size\n",
    "            nodes = self.nodes[cursor:cursor+batch_size]\n",
    "            node_groups.append(nodes)\n",
    "        for nodes in node_groups:\n",
    "            x = []\n",
    "            y = []\n",
    "            z = []\n",
    "            for node in self.nodes:\n",
    "                x_item = node.board.getBoard()\n",
    "                # 盤面の状態をndarray型に変換し、全ての要素を+1して、float型に変換\n",
    "                x_item = np.array(x_item)           # ndarray型に変換\n",
    "                if node.board.player == -1:\n",
    "                    x_item = x_item * (-1)\n",
    "                x_item = x_item + 1\n",
    "                x_item.tolist()                     # リストに変換\n",
    "                x.append(x_item)\n",
    "                # 教師信号は、mcTreeにおいて、選択されたノードに対する、子ノードの選択確率(ここでは、石を置いた場所に確率を割り当てる)\n",
    "                y.append(node.probabilities())\n",
    "                # 報酬は、ノードの評価値を割り当てる\n",
    "                z.append(node.score/node.visits)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "            zs.append(z)\n",
    "        xs = np.array(xs, dtype=np.float32)\n",
    "        # 方策をndarray型に変換\n",
    "        ys = np.array(ys, dtype=np.float32)\n",
    "        # 報酬(勝率)をndarray型に変換\n",
    "        zs = np.array(zs, dtype=np.float32)\n",
    "        # ミニバッチのデータをpytorchのテンソルに変換\n",
    "        inputs = trans(xs)\n",
    "        target1s = trans(ys)\n",
    "        target2s = trans(zs)\n",
    "        return inputs, target1s, target2s\n",
    "    def fshow(self, filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            # 盤面をファイルに書き込む\n",
    "            idx = 0\n",
    "            for path in self.path:\n",
    "                f.write(\"---------<< path #\")\n",
    "                f.write(str(idx))\n",
    "                f.write(\" >>-----------\\n\")\n",
    "                for node in path:\n",
    "                    node.board.fshow(filename)\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **方策ネットワーククラスの実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class p_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 親クラスのnn.Moduleを呼び出し\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, padding=2, padding_mode='replicate'),    # out_channelsは欲しい特徴マップの数\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 16, kernel_size=5, padding=2, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 4, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 1, kernel_size=1),\n",
    "            nn.Softmax(dim=1)               # ミニバッチの各データセットごとに確率分布に変換\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 64),              # 全結合層\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "        )\n",
    "    # 盤面の状態を入力として、方策を返す\n",
    "    def forward(self, input):\n",
    "        hidden = self.features(input)\n",
    "        hidden = hidden.view(hidden.size(0),-1)  # x.size(0)は例えばnum_batches\n",
    "        output = self.classifier(hidden)\n",
    "        return output\n",
    "    # 実際の石の置き場所と方策との交差エントロピー誤差を損失関数とする\n",
    "    def backprop(self, output, optimizer, criterion, target):\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # return loss(output, target)\n",
    "    # 盤面を入力したときの、方策を計算する関数\n",
    "    # ネットワークは、ミニバッチ学習用に設計されているため、1つのデータセットを入力する場合には、空のデータセットを追加する\n",
    "    def predict(self, board):\n",
    "        # 盤面の状態をndarray型に変換し、全ての要素を+1して、float型に変換\n",
    "        x = board.getBoard()\n",
    "        x = np.array(x, dtype=np.float32)           # ndarray型に変換\n",
    "        if board.player == -1:\n",
    "            x = x * (-1)\n",
    "        x = x + 1\n",
    "        x = trans(x)\n",
    "        pred = self.forward(x)\n",
    "        # predを8x8の配列に変換\n",
    "        return pred.view(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **価値ネットワーククラスの実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class v_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(v_net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, padding=2, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 16, kernel_size=5, padding=2, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 4, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 1, kernel_size=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    # 順伝播 \n",
    "    def forward(self, input):\n",
    "        hidden = self.features(input)\n",
    "        hidden = hidden.view(hidden.size(0),-1)\n",
    "        output = self.value(hidden)\n",
    "        return output\n",
    "    # 実際の勝率と勝率の予測値との二乗誤差を損失関数とする\n",
    "    def backprop(self, output, optimizer, criterion, target):\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "    # 盤面を入力したときの、勝率を予測する関数\n",
    "    def predict(self, board, key):\n",
    "        # 盤面の状態をndarray型に変換し、全ての要素を+1して、float型に変換\n",
    "        x = board.put(key[1],key[0]).getBoard()         # key[1]: y, key[0]: x に石を置いた盤面を取得\n",
    "        x = np.array(x, dtype=np.float32)                                 # ndarray型に変換\n",
    "        if board.player == -1:\n",
    "            x = x * (-1)\n",
    "        x = x + 1\n",
    "        input = trans(x)\n",
    "        # 予測値を計算\n",
    "        pred = self.forward(input)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **学習の一例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ooooooox|\n",
      "|ooooooxx|\n",
      "|oooooxox|\n",
      "|ooooxoox|\n",
      "|ooooooox|\n",
      "|oooxooox|\n",
      "|ooooxxox|\n",
      "|xxxxxxoo|\n",
      "player: x\n",
      "Game Over\n",
      "White(◯) wins!\n",
      "Game Over\n",
      "White(◯) wins!\n",
      "<__main__.MCTree object at 0x1205dde20>\n",
      "<__main__.Player object at 0x120699fd0>\n"
     ]
    }
   ],
   "source": [
    "# ネットワークの初期化\n",
    "policy = p_net()             # 方策NNを作成\n",
    "Q_value = v_net()            # 行動価値NNを作成\n",
    "\n",
    "# 対戦の準備\n",
    "board = Board()             # 盤面を初期化\n",
    "mctree = MCTree(board)      # モンテカルロ探索木を初期化\n",
    "\n",
    "# 自己対戦プレイヤーを作成\n",
    "player = Player(policy, Q_value)      \n",
    "player.play1batch(mctree, 1)  # サンプルサイズ分、自己対戦を行い、学習する\n",
    "# print(mctree)\n",
    "# print(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークの初期化\n",
    "policy = p_net()             # 方策NNを作成\n",
    "Q_value = v_net()            # 行動価値NNを作成\n",
    "\n",
    "# オプティマイザ、エポック数、バッチサイズ、モンテカルロ・シミュレーションのサンプルサイズを設定\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr = 0.001, weight_decay = 5e-4)\n",
    "Q_value_optimizer = optim.Adam(Q_value.parameters(), lr = 0.001, weight_decay = 5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "batch_size = 64             # バッチサイズ\n",
    "sample_size = 200          # サンプルサイズ(モンテカルロ・シミュレーションで確率分布を求めるためのサンプル数)  *スケジューリングで変更したい\n",
    "\n",
    "# 対戦の準備\n",
    "board = Board()             # 盤面を初期化\n",
    "mctree = MCTree(board)      # モンテカルロ探索木を初期化\n",
    "\n",
    "# 自己対戦プレイヤーを作成\n",
    "player = Player(policy, Q_value)      \n",
    "player.play1batch(mctree, sample_size)  # サンプルサイズ分、自己対戦を行い、学習する\n",
    "player.fshow(\"log/train_00.txt\")\n",
    "# player.sort_nodes()         # プレイしたノードを訪問回数の降順にソート\n",
    "\n",
    "# ミニバッチに分割されたデータを作成\n",
    "Data = player.prepare(batch_size) \n",
    "\n",
    "# ミニバッチ学習\n",
    "for input, policy_target, reward_target in Data:\n",
    "    # 順伝播\n",
    "    policy_output = policy.forward(input)\n",
    "    reward_output = Q_value.forward(input)\n",
    "    # 逆伝播\n",
    "    policy.backprop(policy_output, policy_optimizer, criterion, policy_target)\n",
    "    Q_value.backprop(reward_output, Q_value_optimizer, criterion, reward_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
