{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **概要**\n",
    "\n",
    "AlphaZeroのアルゴリズムは以下のような手順で構成される：\n",
    "\n",
    "1. 自己対戦による学習: AlphaZeroは、自分自身と対戦しながら学習する。ランダムな手法からスタートし、強化学習アルゴリズムを用いて次第に強くなる。\n",
    "\n",
    "2. モンテカルロ木探索: AlphaZeroは、モンテカルロ木探索（MCTS）を使用してゲームのツリーを探索する。これにより、最適な手を見つけるために局面を評価し、次の手を決定する。\n",
    "\n",
    "3. ニューラルネットワーク: AlphaZeroは、ゲームの状態を評価するためのニューラルネットワークを使用する。このニューラルネットワークは、ゲームの局面を入力とし、局面の価値を出力する。価値は、勝率や局面の良さなどを示す。\n",
    "\n",
    "4. 強化学習: AlphaZeroは、報酬を最大化するようにニューラルネットワークを調整することで学習します。報酬は、ゲームの勝利や敗北などによって与えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ライブラリ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# pytorchのライブラリをインポート\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 対戦状況を可視化するためのライブラリをインポート\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "trans = torchvision.transforms.ToTensor()  # データをpytorchのテンソルに変換する関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **オセロのルールを反映させたクラス**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オセロの状態を表すクラス(構造体)\n",
    "# 8x8の盤面を持ち、各マスには黒か白か空かのいずれかが入る\n",
    "# 盤面の状態を配列で表現する\n",
    "class Board():\n",
    "    # 8x8の配列を初期化する\n",
    "    def __init__(self):\n",
    "        self.board = [[0 for i in range(8)] for j in range(8)]\n",
    "        self.board[3][3] = 1    # 白(◯)\n",
    "        self.board[4][4] = 1    # 白\n",
    "        self.board[3][4] = -1    # 黒(×)\n",
    "        self.board[4][3] = -1    # 黒\n",
    "        self.player = -1         # 黒が先手\n",
    "    # 盤面の配列を取得する\n",
    "    def getBoard(self):\n",
    "        return self.board\n",
    "    # 盤面の配列をベクトルとして取得する\n",
    "    def getBoardVec(self):\n",
    "        return np.array(self.board).reshape(64).tolist()\n",
    "    # 盤面の状態を表示する\n",
    "    def show(self):\n",
    "        for row in self.board:\n",
    "            print(\"|\", end=\"\")\n",
    "            for cell in row:\n",
    "                if cell == 0:\n",
    "                    print(\" \", end=\"\")  # end=\"\" で改行しない\n",
    "                elif cell == 1:\n",
    "                    print(\"o\", end=\"\")\n",
    "                elif cell == -1:\n",
    "                    print(\"x\", end=\"\")\n",
    "            print(\"|\")                     # 一行表示して改行\n",
    "        print(\"player: o\") if self.player == 1 else print(\"player: x\")\n",
    "    # ファイルに盤面の状態を書き込む\n",
    "    def fshow(self, filename):\n",
    "        with open(filename, \"a\") as f:\n",
    "            for row in self.board:\n",
    "                f.write(\"|\")\n",
    "                for cell in row:\n",
    "                    if cell == 0:\n",
    "                        f.write(\" \")\n",
    "                    elif cell == 1:\n",
    "                        f.write(\"o\")\n",
    "                    elif cell == -1:\n",
    "                        f.write(\"x\")\n",
    "                    # f.write(str(cell))\n",
    "                f.write(\"|\\n\")\n",
    "            if self.player == 1:\n",
    "                f.write(\"player: o\")\n",
    "            else:\n",
    "                f.write(\"player: x\")\n",
    "    # 置く場所が盤面上にあるか判定する\n",
    "    def outofBoard(self, x, y):\n",
    "        return x < 0 or x >= 8 or y < 0 or y >= 8\n",
    "    # (x, y)に石を置くとき、(dx, dy)方向にひっくり返す石があるか判定する\n",
    "    def canPutDir(self, x, y, dx, dy):\n",
    "        i = 1\n",
    "        if self.outofBoard(x + dx, y + dy):\n",
    "            return False\n",
    "        else:\n",
    "            while self.board[y + dy * i][x + dx * i] == -(self.player):    # 相手の石が続く限り\n",
    "                i += 1\n",
    "                if self.outofBoard(x + dx * i, y + dy * i):                 # 盤面外に出たら\n",
    "                    return False\n",
    "                elif self.board[y + dy * i][x + dx * i] == 0:               # 石がない場所があれば\n",
    "                    return False\n",
    "                elif self.board[y + dy * i][x + dx * i] == self.player:     # 自分の石で挟めるなら\n",
    "                    return True\n",
    "    # (x, y)に石を置けるか判定する\n",
    "    def canPut(self, x, y):\n",
    "        if not self.outofBoard(x,y) and self.board[y][x] != 0:                                           # すでに石が置かれている\n",
    "            return False\n",
    "        # 周囲8方向を調べる; 1方向でもひっくり返せるなら置ける\n",
    "        for dx in range(-1, 2):\n",
    "            for dy in range(-1, 2):\n",
    "                if dx == 0 and dy == 0: \n",
    "                    continue\n",
    "                if self.canPutDir(x, y, dx, dy):                             # (dx, dy)方向にひっくり返せる石がある\n",
    "                    return True\n",
    "        return False\n",
    "    # (x, y)に石を置いたときの盤面クラスを返す\n",
    "    def put(self, x, y):\n",
    "        newBoard = Board()\n",
    "        newBoard.board = [row[:] for row in self.board] # 盤面をコピー\n",
    "        newBoard.player = -self.player   # プレイヤーを交代\n",
    "        newBoard.board[y][x] = self.player  # 石を置く\n",
    "        # ひっくり返せる石を全てひっくり返す\n",
    "        for dx in range(-1, 2):\n",
    "            for dy in range(-1, 2):\n",
    "                if dx == 0 and dy == 0:\n",
    "                    continue\n",
    "                if self.canPutDir(x, y, dx, dy):\n",
    "                    i = 1\n",
    "                    while newBoard.board[y + dy * i][x + dx * i] == -self.player:\n",
    "                        newBoard.board[y + dy * i][x + dx * i] = self.player    # 石をひっくり返す\n",
    "                        i += 1\n",
    "        return newBoard\n",
    "    # 石を置ける場所があるか判定する\n",
    "    def canPlay(self):\n",
    "        for y in range(8):\n",
    "            for x in range(8):\n",
    "                if self.canPut(x, y):\n",
    "                    return True\n",
    "        return False\n",
    "    # 石を置ける場所と置いた場合の盤面クラスを辞書型で返す\n",
    "    def choices(self):          # 動的計画法において、for choice in board.choices(): で呼び出す\n",
    "        choices = {}\n",
    "        for y in range(8):\n",
    "            for x in range(8):\n",
    "                if self.canPut(x, y):\n",
    "                    choices[(x, y)] = self.put(x, y)\n",
    "        return choices\n",
    "    # 石を置ける場所がなく場合にプレイヤーを交代するか判定し、交代する (交代する場合は新たな盤面を返す)\n",
    "    def passPlayer(self):   # Board.passPlayer()で呼び出す\n",
    "        if not self.canPlay():\n",
    "            # 新たな盤面を作成し、プレイヤーを交代する\n",
    "            newBoard = Board()\n",
    "            newBoard.board = [row[:] for row in self.board] # 盤面をコピー\n",
    "            newBoard.player = -self.player\n",
    "            return newBoard\n",
    "        else:\n",
    "            return self\n",
    "    # 盤面が等しいか判定する\n",
    "    def isEq(self, other):\n",
    "        return self.board == other.board and self.player == other.player\n",
    "    # 盤面を比較し、打った手を返す\n",
    "    def diff(self, other):\n",
    "        for y in range(8):\n",
    "            for x in range(8):\n",
    "                if self.board[y][x] != other.board[y][x]:\n",
    "                    return (x, y)\n",
    "        return None\n",
    "    # 勝負がついたか判定する\n",
    "    def isEnd(self):\n",
    "        if not self.canPlay() and not self.passPlayer().canPlay():\n",
    "            # print(\"Game Over\")\n",
    "            # self.winner()\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    # 石の数を数え、勝敗を判定する\n",
    "    def counter(self, num):\n",
    "        tot = 0\n",
    "        for row in self.board:\n",
    "            for cell in row:\n",
    "                if cell == num:\n",
    "                    tot += 1\n",
    "        return tot\n",
    "    # 勝者を返す\n",
    "    def winner(self):\n",
    "        if not self.isEnd():\n",
    "            return\n",
    "        # print(\"Game Over\")\n",
    "        black = self.counter(-1)\n",
    "        white = self.counter(1)\n",
    "        if black > white:\n",
    "            # print(\"Black(×) wins!\")\n",
    "            return -1\n",
    "        elif black < white:\n",
    "            # print(\"White(◯) wins!\")\n",
    "            return 1\n",
    "        else:\n",
    "            # print(\"Draw\")\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **実際に人間同士でプレイしてみる**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playHtoH(filename):\n",
    "    # ゲームをプレイする\n",
    "    board = Board()\n",
    "    # ログファイルを初期化\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(time.strftime(\"%Y/%m/%d %H:%M\") + \"\\n\")\n",
    "        f.close()\n",
    "    x, y = 0, 0\n",
    "    while not board.isEnd():\n",
    "        time.sleep(1.0)                                     # 0.5秒待つ\n",
    "        clear_output()                                      # 画面をクリア\n",
    "        board.show()                                        # 盤面を表示\n",
    "        board.fshow(filename)                            # 盤面をファイルに書き込む\n",
    "        if board.canPlay():\n",
    "            print(\"Put coordinate x, y: \", end=\"\")\n",
    "            x, y = map(int, input().split())                # 入力を受け取る\n",
    "            print(\"(\", x, \", \", y,\")\")                      # 入力を表示\n",
    "            if board.canPut(x, y):\n",
    "                board = board.put(x, y)\n",
    "            else:\n",
    "                print(\"Can't put\")\n",
    "        else:\n",
    "            board = board.passPlayer()                      # 石を置けない場合はプレイヤーを交代\n",
    "    time.sleep(1.0)\n",
    "    clear_output()\n",
    "    board.show()\n",
    "    board.fshow(filename)\n",
    "    board.winner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        |\n",
      "|   o    |\n",
      "|   oo   |\n",
      "| ooooo  |\n",
      "|   ooo  |\n",
      "|   ooo  |\n",
      "|        |\n",
      "|        |\n",
      "player: x\n"
     ]
    }
   ],
   "source": [
    "playHtoH(\"log/demoPlay_00.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **探索アルゴリズム**\n",
    "\n",
    "**Monte-Carlo-Tree-Search(MCTS)**による学習アルゴリズムを実装する。\n",
    "\n",
    "### **MCTSのアルゴリズム**\n",
    "\n",
    "参考文献：[Monte Carlo Tree Search - Wikipedia](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)\n",
    "\n",
    "モンテカルロ木探索は4つのステップからなる。\n",
    "\n",
    "Nodeは、ゲームの状態を表し、プレイアウトの回数と勝利の回数を保持している。\n",
    "\n",
    "1. 選択：Root Node から始めて、Leaf Node にたどり着くまで、子ノードを選択し続ける。Root Node が現在のゲームの状態で、Leaf Node はシミュレーションが行われていないノード。より有望な方向に木が展開していくように、子ノードの選択を片寄らせる方法を採用するため、以下の Alpha Zero 指標を用いる。\n",
    "2. 展開：Leaf Node が勝負を決するノードでない限り、Leaf Node から有効手の子ノードの中から Child Node を1つ選ぶ。\n",
    "3. シミュレーション：Child Node からプレイアウトを行う。Alpha Zero 指標に基づいてプレイアウトを実行する。\n",
    "4. バックプロパゲーション：Child Node から Root Node へのパスに沿って、プレイアウトの結果を伝搬する。\n",
    "\n",
    "### **Alpha Zero 指標**\n",
    "\n",
    "Alpha Zero 指標は、探索と活用のバランスを取る指標である。\n",
    "\n",
    "$ Q(s, a) + U(s, a) $\n",
    "\n",
    "が最大となる行動を選択する。ここで、$Q(s, a)$ は状態 $s$ で行動 $a$ を取った時の行動価値関数 $(=V(s'))$、$U(s, a)$ は探索項である。\n",
    "\n",
    "$ U(s, a) = c(s) P(s, a) \\frac{\\sqrt{\\sum_{b} N(s, b)}}{1 + N(s, a)} $\n",
    "\n",
    "ここで、$c(s)$ は下記の式で定まる数で、$P(s, a)$ は方策ネットワークによる行動 $a$ の選択確率、$N(s, a)$ は状態 $s$ で行動 $a$ を選択した回数、$\\sum_{b} N(s, b)$ は状態 $s$ に到達した回数である。\n",
    "\n",
    "$ c(s) = c_{init} + \\log{\\frac{1 + N(s) + c_{base}}{c_{base}}} $\n",
    "\n",
    "ここで、$c_{base}$ はハイパーパラメータであり、$N(s)$ は状態 $s$ に到達した回数である。\n",
    "\n",
    "### **Q関数の更新**\n",
    "\n",
    "チェスは、遷移確率が one-hot つまり $P(s'|s,a)=1$ であるため、状態価値関数 $V(s)$ と行動価値関数 $Q(s, a)$ は同じものとして扱うことができる。したがって、\n",
    "盤面の状態を64次元のベクトル(とる値は ${-1,0,1}$ )で表現し、それに対して有利不利を $[-1,1]$ の範囲で出力する状態行動価値関数 $Q(s,a)=V(s')$ を学習する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MCT Node クラスの実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte-Carlo Tree のノードを表すクラス\n",
    "class MCTSNode:\n",
    "    def __init__(self, board, x=0, y=0):\n",
    "        self.board = board          # Boardクラス\n",
    "        self.parent = None          # 親ノード\n",
    "        self.children = []          # 子ノード\n",
    "        self.visits = 0             # ノードを訪れた回数\n",
    "        self.score = 0              # ノードの評価値\n",
    "        self.x = x                  # 石を置いた場所\n",
    "        self.y = y\n",
    "    # ノードの展開(子ノードを追加)\n",
    "    def expand(self):\n",
    "        if self.board.isEnd():          # ゲーム終了時は展開しない\n",
    "            return\n",
    "        elif len(self.children) > 0:    # 既に展開済みの場合は展開しない\n",
    "            return\n",
    "        else:\n",
    "            choices = self.board.choices()              # 石を置ける場所と置いた場合の盤面クラスを取得(Boardクラスのchoicesメソッド)\n",
    "            for choice, board in choices.items():       # 石を置ける場所全てについて、子ノードに追加\n",
    "                x, y = choice\n",
    "                node = MCTSNode(board, x, y)\n",
    "                node.parent = self\n",
    "                self.children.append(node)\n",
    "    # 子ノードの選択確率を計算し、石を置いた場所にその確率を当てはめた二次元配列を返す\n",
    "    def probabilities(self, flag):\n",
    "        probs = [[0 for i in range(8)] for j in range(8)]\n",
    "        choices = self.board.choices()\n",
    "        for child in self.children:\n",
    "            board = child.board\n",
    "            x, y = child.x, child.y\n",
    "            probs[y][x] = child.visits/self.visits\n",
    "            # for x, y in choices:    # 石を置いた場所を取得\n",
    "            #     print(key, board)     # デバグ用\n",
    "            #     board = choices[(x, y)]\n",
    "            #     if child.board.isEq(board):             # 石を置いた場所が等しい場合\n",
    "            #         probs[y][x] = child.visits/self.visits\n",
    "        # 確率の合計がほぼ1であることを確認し、あまりにも差がある場合には、エラーを出力\n",
    "        v = sum([sum(prob) for prob in probs])\n",
    "        if abs(v-1) > 0.01 and abs(v) > 0.01:\n",
    "            print(\"Error: sum(prob) != 1\")\n",
    "            print(np.array(probs).reshape(64).tolist())\n",
    "        # 二次元配列を返すか、フラグが立っている場合は一次元配列に変換して返す\n",
    "        if flag==True:\n",
    "            return np.array(probs).reshape(64).tolist()\n",
    "        else:\n",
    "            return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AlphaZero 指標の計算関数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mctsNodeで、choiceを選択することの評価値(行動価値関数と探索項の和)を返す\n",
    "def alpha_zero_score(mctsNode, key, child, mcTree, policy, Q_value, flag=True):\n",
    "    # ハイパーパラメータ\n",
    "    c_base = 19652\n",
    "    c_init = 1.25\n",
    "    # 行動価値関数の値を計算\n",
    "    value = Q_value.predict(mctsNode.board, key)\n",
    "    value = value.item()                                # tensorを数値に変換\n",
    "    # ノードの探索項を計算\n",
    "    N = mctsNode.visits                                 # mctsNodeを訪れた回数\n",
    "    # childNode = mcTree.move_cop(key)                    # keyの手を打ったときのノードを取得\n",
    "    Nc = child.visits                               # mctsNodeからchoiceを選択した回数\n",
    "    C = c_init + np.log((N + c_base + 1) / c_base)      # ノードの訪問回数で決まる係数\n",
    "    x, y = key                                          # keyの手を打った場所\n",
    "    p = policy.predict(mctsNode.board)[y][x]\n",
    "    p = p.item()                                        # tensorを数値に変換\n",
    "    exploration = C * np.sqrt(N) / (1 + Nc) * p          # 探索項\n",
    "    if flag:\n",
    "        # ノードの評価値と探索項の和を返す\n",
    "        return value + exploration\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC Tree クラスの実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTree:\n",
    "    def __init__(self, board):\n",
    "        self.root = MCTSNode(board)                 # ルートノードを作成\n",
    "        self.current = self.root                    # 現在のノード\n",
    "    # ノードの選択(flag=True: 探索を行う, flag=False: 探索を行わない)\n",
    "    def select(self, Policy, Q_value, flag=True):\n",
    "        node = self.current\n",
    "        node_max = self.current\n",
    "        node.expand()\n",
    "        if len(node.children) == 0:\n",
    "            return node\n",
    "        choices = self.current.board.choices()              # 石を置ける場所と置いた場合の盤面を取得\n",
    "        max_score = -math.inf\n",
    "        for child in node.children:\n",
    "            score = alpha_zero_score(node, (child.x, child.y), child, self, Policy, Q_value, flag)\n",
    "            if score > max_score:\n",
    "                #print(\"score: \", score)\n",
    "                max_score = score\n",
    "                node_max = child\n",
    "        # print(\"start\")\n",
    "        # for choice, board in choices.items():\n",
    "        #     for child in node.children:\n",
    "        #         if child.board.isEq(board):\n",
    "        #             score = alpha_zero_score(node, choice, self, Policy, Q_value, flag)\n",
    "        #             print(\"score: \", score)\n",
    "        #             if score > max_score:\n",
    "        #                 print(\"True\")\n",
    "        #                 max_score = score\n",
    "        #                 node = child\n",
    "        # time.sleep(1.0)    # 0.5秒待つ\n",
    "        # clear_output()    # 画面をクリア\n",
    "        # print(\"I choose: \")    # デバグ用\n",
    "        # print(node_max.x, node_max.y)    # デバグ用\n",
    "        # node_max.board.show()    # デバグ用\n",
    "        return node_max\n",
    "    # ノードの移動(クラスをコピーせず、現在のノードを変更する)\n",
    "    def move_cur(self, key):                        # key: 石を置く場所を指定\n",
    "        # 現在のノードに対して、keyに石を置いた場合の盤面を辞書のキーから検索\n",
    "        choices = self.current.board.choices()              # 石を置ける場所と置いた場合の盤面を取得\n",
    "        for choice, board in choices.items():\n",
    "            if choice == key:                               # keyに石を置いた場合の盤面クラスを子ノードから見つけたら\n",
    "                for child in self.current.children:\n",
    "                    if child.board.isEq(board):                # 盤面クラスの一致を確認\n",
    "                        self.current = child\n",
    "                        return\n",
    "    # ノードの移動(クラスをコピーして、現在のノードを変更する)\n",
    "    def move_cop(self, key):                       # key: 石を置く場所を指定\n",
    "        # 現在のノードに対して、keyに石を置いた場合の盤面を辞書のキーから検索\n",
    "        choices = self.current.board.choices()              # 石を置ける場所と置いた場合の盤面を取得\n",
    "        for choice, board in choices.items():\n",
    "            if choice == key:                               # keyに石を置いた場合の盤面クラスを子ノードから見つけたら\n",
    "                for child in self.current.children:\n",
    "                    if child.board.isEq(board):                # 盤面クラスの一致を確認\n",
    "                        self.current = child\n",
    "                        return child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **自己対戦クラスの実装**\n",
    "\n",
    "プレイヤークラスのメソッドは以下の通りです。\n",
    "\n",
    "#### **MC Tree の初期化**\n",
    "\n",
    "#### **経験の獲得**\n",
    "\n",
    "今回は、行動毎に報酬を得るわけではなく、１ゲームをプレーした後に報酬を得ることに注意が必要である。さらには、バッチサイズ分ゲームをプレーして、それに対する\n",
    "\n",
    "#### **MC Tree の更新**\n",
    "\n",
    "実際のプレイヤーの行動とそれに対する報酬(勝利:1, 敗北:-1, 引き分け:0)を受け取り、MC Tree に追加します。\n",
    "\n",
    "#### **状態行動価値関数の初期化**\n",
    "\n",
    "状態行動価値関数 $Q_{\\theta}(s_t, a_t)$ について、初期化する。\n",
    "\n",
    "#### **方策の更新**\n",
    "\n",
    "基本的には、マルチステップ方策評価・マルチステップ方策更新を行う。その際には、Greedy Policy とするか、$\\epsilon$-Greedy Policy 、あるいは Softmax Policy とするかを指定する。\n",
    "\n",
    "#### **行動の選択**\n",
    "自分のターンが来たら、方策に従って行動を選択する。決定論的な方策を取るか、確率的な方策を取るかは、方策の種類次第になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **データセットクラスの実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, xs, ys, zs): # xs: 盤面の状態が並んだリスト, ys: 方策が並んだリスト, zs: 報酬が並んだリスト\n",
    "        self.input = xs\n",
    "        self.target_prob = ys\n",
    "        self.target_rew = zs\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    def __getitem__(self, idx):\n",
    "        # 盤面の状態を、in_channels=1, 8x8のテンソルに変換\n",
    "        input = torch.tensor(self.input[idx], dtype = torch.float32)\n",
    "        input = input.unsqueeze(0)\n",
    "        # 方策を、長さ64の一次元テンソルに変換\n",
    "        target_prob = torch.tensor(self.target_prob[idx], dtype = torch.float32)\n",
    "        # 報酬を、スカラー値に変換\n",
    "        target_rew = torch.tensor(self.target_rew[idx], dtype = torch.float32)\n",
    "        return input, target_prob, target_rew\n",
    "    \n",
    "# dataset = MyDataset(xs, ys, zs)   # データセットを作成\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)   # データローダーを作成\n",
    "# for inputs, target1s, target2s in dataloader:  # データローダーからミニバッチを取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, Policy, Q_value):\n",
    "        self.Policy = Policy            # 方策NN\n",
    "        self.Q_value = Q_value          # 行動価値NN            \n",
    "        self.path = [[]]                # プレイしたノード(MCTSNodeクラス)の履歴\n",
    "        self.nodes = []                 # ミニバッチ全体で一度でもプレイしたノード(MCTSNodeクラス)のリスト\n",
    "        self.cursor = 0                 # ミニバッチにおける現在のインデックス\n",
    "        self.reward = [0,0]             # [黒の報酬、白の報酬](勝利: 1, 引き分け: 0.5, 敗北: 0)\n",
    "    # 1手プレイする\n",
    "    def oneplay(self, mctree, flag=True):\n",
    "        # モンテカルロ木探索\n",
    "        if len(mctree.current.children)==0:             # 子ノードがない場合は展開\n",
    "            mctree.current.expand()\n",
    "            if len(mctree.current.children)==0:         # 打てる手がない場合はパス\n",
    "                mctree.current.board = mctree.current.board.passPlayer()\n",
    "                return mctree\n",
    "        ans = mctree.select(self.Policy, self.Q_value, flag)  # ノードの選択\n",
    "        # time.sleep(1.0)                                 # 0.5秒待つ\n",
    "        # clear_output()                   \n",
    "        #                # 画面をクリア\n",
    "        # print(\"selected:\")\n",
    "        # ans.board.show()                                # 盤面を表示(デバッグ用)\n",
    "        mctree.current = ans    # ノードの選択と現ノードの変更\n",
    "        # time.sleep(0.2)                                     # 0.5秒待つ\n",
    "        # clear_output()                                      # 画面をクリア\n",
    "        # mctree.current.board.show()                                 # 盤面を表示(デバッグ用)\n",
    "        self.path[self.cursor].append(mctree.current)                # 選択したノードを記録\n",
    "        # self.nodesに進んだ先のノードがなければ追加\n",
    "        if mctree.current not in self.nodes:\n",
    "            self.nodes.append(mctree.current)\n",
    "        return mctree\n",
    "    # 1ゲーム自己対戦する\n",
    "    def play1game(self, mctree):\n",
    "        mctree.current = mctree.root\n",
    "        while not mctree.current.board.isEnd():\n",
    "            mctree = self.oneplay(mctree)\n",
    "            #print(\"played\")\n",
    "            #mctree.current.board.show()\n",
    "            # print(\"p\",end=\"\")\n",
    "        board = mctree.current.board\n",
    "        #board.show()\n",
    "        print(\"winner: \", board.winner())\n",
    "        if board.winner() == -1:\n",
    "            self.reward[0] = 1\n",
    "            self.reward[1] = -1\n",
    "        elif board.winner() == 1:\n",
    "            self.reward[0] = -1\n",
    "            self.reward[1] = 1\n",
    "        else:\n",
    "            self.reward[0] = 0\n",
    "            self.reward[1] = 0\n",
    "        # else:\n",
    "        #     self.reward[0] += 0.5\n",
    "        #     self.reward[1] += 0.5\n",
    "    # プレイしたノードの訪問回数と評価値を更新する\n",
    "    def backup(self):\n",
    "        for node in self.path[self.cursor]:                  # プレイしたノード全てについて\n",
    "            node.visits += 1\n",
    "            if node.board.player == -1:          # そのノードでプレイしたのが黒の場合\n",
    "                node.score += self.reward[1]\n",
    "            else:                               # そのノードでプレイしたのが白の場合\n",
    "                node.score += self.reward[0]\n",
    "            # print(\"score: \", node.score)\n",
    "            # print(\"visits: \", node.visits)\n",
    "            # node.board.show()\n",
    "            # print(\"b\",end=\"\")\n",
    "    # モンテカルロ・シミュレーションのサンプルサイズ分、1ゲーム自己対戦・backupする\n",
    "    def play1batch(self, mctree, sample_size):\n",
    "        for i in range(sample_size):\n",
    "            k = 1\n",
    "            if sample_size > 2:\n",
    "                k = sample_size // 2\n",
    "            # if i % k == 0:\n",
    "            #     print(\"game\", i)\n",
    "            print(\"game\", i)\n",
    "            self.play1game(mctree)\n",
    "            self.backup()\n",
    "            if i != sample_size - 1:\n",
    "                self.cursor += 1\n",
    "                self.path.append([])\n",
    "    # self.nodesを、visit回数の降順にソートする\n",
    "    def sort_nodes(self):\n",
    "        self.nodes.sort(key=lambda x: x.visits, reverse=True)\n",
    "    # データをロードする(盤面、手の選択確率、報酬)\n",
    "    def prepare(self, batch_size):\n",
    "        # データセットをリストで作成\n",
    "        xs = []  # 盤面の状態(入力)\n",
    "        ys = []  # 方策(出力)\n",
    "        zs = []  # 報酬\n",
    "        for node in self.nodes:\n",
    "            x_item = node.board.getBoard()\n",
    "            # 盤面の状態をndarray型に変換し、全ての要素を+1して、float型に変換\n",
    "            x_item = np.array(x_item)           # ndarray型に変換\n",
    "            if node.board.player == -1:\n",
    "                x_item = x_item * (-1)\n",
    "            x_item = x_item + 1\n",
    "            x_item.tolist()                     # リストに変換\n",
    "            xs.append(x_item)\n",
    "            # 教師信号は、mcTreeにおいて、選択されたノードに対する、子ノードの選択確率(ここでは、石を置いた場所に確率を割り当てた64次元のベクトル)\n",
    "            ys.append(node.probabilities(True))\n",
    "            # 報酬は、ノードの評価値を割り当てる\n",
    "            zs.append(node.score/node.visits)\n",
    "        dataset = MyDataset(xs, ys, zs)   # pytorchに利用可能な形で、データセットを作成\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)  # データローダーを作成\n",
    "        return dataloader\n",
    "    def fshow(self, filename):\n",
    "        # 盤面をファイルに書き込む\n",
    "        idx = 0\n",
    "        for path in self.path:\n",
    "            if len(path) > 0:\n",
    "                with open(filename, \"a\") as f:\n",
    "                    f.write(\"---------<< path #\")\n",
    "                    f.write(str(idx))\n",
    "                    f.write(\" >>-----------\\n\")\n",
    "                    f.close()\n",
    "                path[-1].board.fshow(filename)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **方策ネットワーククラスの実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class p_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 親クラスのnn.Moduleを呼び出し\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3, padding=1, padding_mode='replicate'),    # out_channelsは欲しい特徴マップの数\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 16, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 4, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 1, kernel_size=1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 64),              # 全結合層\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Softmax(dim=1)               # ミニバッチの各データセットごとに確率分布に変換\n",
    "        )\n",
    "    # 盤面の状態を入力として、方策を返す\n",
    "    def forward(self, input):\n",
    "        hidden = self.features(input)\n",
    "        hidden = hidden.view(hidden.size(0),-1)  # x.size(0)は例えばnum_batches\n",
    "        output = self.classifier(hidden)         # 64次元のベクトルとして方策を返す\n",
    "        return output\n",
    "    # 実際の石の置き場所と方策との交差エントロピー誤差を損失関数とする\n",
    "    def backprop(self, output, optimizer, criterion, target):\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # return loss(output, target)\n",
    "    # 盤面を入力したときの、方策を計算する関数\n",
    "    # ネットワークは、ミニバッチ学習用に設計されているため、1つのデータセットを入力する場合には、空のデータセットを追加する\n",
    "    def predict(self, board):\n",
    "        # 盤面の状態をndarray型に変換し、全ての要素を+1して、float型に変換\n",
    "        x = board.getBoard()\n",
    "        x = np.array(x, dtype=np.float32)           # ndarray型に変換\n",
    "        if board.player == -1:\n",
    "            x = x * (-1)\n",
    "        x = x + 1\n",
    "        x = trans(x)\n",
    "        pred = self.forward(x)\n",
    "        # predを8x8の配列に変換\n",
    "        return pred.view(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **価値ネットワーククラスの実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class v_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(v_net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=7, padding=3, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 16, kernel_size=5, padding=2, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 4, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 1, kernel_size=1)\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    # 順伝播 \n",
    "    def forward(self, input):\n",
    "        hidden = self.features(input)\n",
    "        hidden = hidden.view(hidden.size(0),-1)\n",
    "        output = self.value(hidden)\n",
    "        return output\n",
    "    # 実際の勝率と勝率の予測値との二乗誤差を損失関数とする\n",
    "    def backprop(self, output, optimizer, criterion, target):\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "    # 盤面を入力したときの、勝率を予測する関数\n",
    "    def predict(self, board, key):\n",
    "        # 盤面の状態をndarray型に変換し、全ての要素を+1して、float型に変換\n",
    "        x = board.put(key[1],key[0]).getBoard()         # key[1]: y, key[0]: x に石を置いた盤面を取得\n",
    "        x = np.array(x, dtype=np.float32)                                 # ndarray型に変換\n",
    "        if board.player == -1:\n",
    "            x = x * (-1)\n",
    "        x = x + 1\n",
    "        input = trans(x)\n",
    "        # 予測値を計算\n",
    "        pred = self.forward(input)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlphaZeroNet(nn.Module):\n",
    "    def __init__(self, board_size, action_space):\n",
    "        super(AlphaZeroNet, self).__init__()\n",
    "        \n",
    "        # Initial convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Residual blocks (19 as per AlphaZero)\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(256) for _ in range(19)]\n",
    "        )\n",
    "\n",
    "        # Policy head\n",
    "        self.policy_conv = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1)\n",
    "        self.policy_bn = nn.BatchNorm2d(1)\n",
    "        self.policy_fc = nn.Linear(board_size * board_size, action_space)\n",
    "\n",
    "        # Value head\n",
    "        self.value_conv = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1)\n",
    "        self.value_bn = nn.BatchNorm2d(1)\n",
    "        self.value_fc1 = nn.Linear(board_size * board_size, 256)\n",
    "        self.value_fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Residual blocks\n",
    "        x = self.res_blocks(x)\n",
    "\n",
    "        # Policy head\n",
    "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
    "        p = p.view(p.size(0), -1)\n",
    "        p = F.log_softmax(self.policy_fc(p))\n",
    "\n",
    "        # Value head\n",
    "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
    "        v = v.view(v.size(0), -1)\n",
    "        v = F.relu(self.value_fc1(v))\n",
    "        v = torch.tanh(self.value_fc2(v))\n",
    "\n",
    "        return p, v\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        return F.relu(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **学習の一例**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **自己対戦アルゴリズムの確認**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 0\n",
      "winner:  1\n"
     ]
    }
   ],
   "source": [
    "# ネットワークの初期化\n",
    "policy = p_net()             # 方策NNを作成\n",
    "Q_value = v_net()            # 行動価値NNを作成\n",
    "net = AlphaZeroNet(8, 64)   # AlphaZeroNetを作成\n",
    "\n",
    "# 対戦の準備\n",
    "board = Board()             # 盤面を初期化\n",
    "mctree = MCTree(board)      # モンテカルロ探索木を初期化\n",
    "\n",
    "# 自己対戦プレイヤーを作成\n",
    "player = Player(policy, Q_value)      \n",
    "player.play1batch(mctree, 1)  # サンプルサイズ分、自己対戦を行い、学習する\n",
    "# print(mctree)\n",
    "# print(player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **学習も含めた実行例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== learn # 0\n",
      "game 0\n",
      "winner:  1\n",
      "game 1\n",
      "winner:  1\n",
      "game 2\n",
      "winner:  -1\n",
      "game 3\n",
      "winner:  1\n",
      "game 4\n",
      "winner:  -1\n",
      "game 5\n",
      "winner:  -1\n",
      "game 6\n",
      "winner:  1\n",
      "game 7\n",
      "winner:  0\n",
      "game 8\n",
      "winner:  0\n",
      "game 9\n",
      "winner:  1\n",
      "game 10\n",
      "winner:  -1\n",
      "game 11\n",
      "winner:  -1\n",
      "game 12\n",
      "winner:  1\n",
      "game 13\n",
      "winner:  1\n",
      "game 14\n",
      "winner:  1\n",
      "game 15\n",
      "winner:  -1\n",
      "game 16\n",
      "winner:  1\n",
      "game 17\n",
      "winner:  -1\n",
      "game 18\n",
      "winner:  -1\n",
      "game 19\n",
      "winner:  1\n",
      "game 20\n",
      "winner:  1\n",
      "game 21\n",
      "winner:  1\n",
      "game 22\n",
      "winner:  -1\n",
      "game 23\n",
      "winner:  1\n",
      "game 24\n",
      "winner:  -1\n",
      "game 25\n",
      "winner:  -1\n",
      "game 26\n",
      "winner:  -1\n",
      "game 27\n",
      "winner:  -1\n",
      "game 28\n",
      "winner:  1\n",
      "game 29\n",
      "winner:  1\n",
      "game 30\n",
      "winner:  1\n",
      "game 31\n",
      "winner:  1\n",
      "game 32\n",
      "winner:  1\n",
      "game 33\n",
      "winner:  -1\n",
      "game 34\n",
      "winner:  -1\n",
      "game 35\n",
      "winner:  -1\n",
      "game 36\n",
      "winner:  1\n",
      "game 37\n",
      "winner:  -1\n",
      "game 38\n",
      "winner:  1\n",
      "game 39\n",
      "winner:  -1\n",
      "game 40\n",
      "winner:  1\n",
      "game 41\n",
      "winner:  1\n",
      "game 42\n",
      "winner:  1\n",
      "game 43\n",
      "winner:  -1\n",
      "game 44\n",
      "winner:  -1\n",
      "game 45\n",
      "winner:  1\n",
      "game 46\n",
      "winner:  1\n",
      "game 47\n",
      "winner:  1\n",
      "game 48\n",
      "winner:  1\n",
      "game 49\n",
      "winner:  1\n",
      "game 50\n",
      "winner:  1\n",
      "game 51\n",
      "winner:  -1\n",
      "game 52\n",
      "winner:  -1\n",
      "game 53\n",
      "winner:  -1\n",
      "game 54\n",
      "winner:  -1\n",
      "game 55\n",
      "winner:  1\n",
      "game 56\n",
      "winner:  -1\n",
      "game 57\n",
      "winner:  1\n",
      "game 58\n",
      "winner:  1\n",
      "game 59\n",
      "winner:  1\n",
      "game 60\n",
      "winner:  -1\n",
      "game 61\n",
      "winner:  1\n",
      "game 62\n",
      "winner:  1\n",
      "game 63\n",
      "winner:  1\n",
      "game 64\n",
      "winner:  1\n",
      "game 65\n",
      "winner:  -1\n",
      "game 66\n",
      "winner:  1\n",
      "game 67\n",
      "winner:  1\n",
      "game 68\n",
      "winner:  -1\n",
      "game 69\n",
      "winner:  1\n",
      "game 70\n",
      "winner:  -1\n",
      "game 71\n",
      "winner:  1\n",
      "game 72\n",
      "winner:  -1\n",
      "game 73\n",
      "winner:  -1\n",
      "game 74\n",
      "winner:  1\n",
      "game 75\n",
      "winner:  -1\n",
      "game 76\n",
      "winner:  -1\n",
      "game 77\n",
      "winner:  1\n",
      "game 78\n",
      "winner:  -1\n",
      "game 79\n",
      "winner:  -1\n",
      "game 80\n",
      "winner:  1\n",
      "game 81\n",
      "winner:  1\n",
      "game 82\n",
      "winner:  -1\n",
      "game 83\n",
      "winner:  -1\n",
      "game 84\n",
      "winner:  -1\n",
      "game 85\n",
      "winner:  1\n",
      "game 86\n",
      "winner:  0\n",
      "game 87\n",
      "winner:  -1\n",
      "game 88\n",
      "winner:  -1\n",
      "game 89\n",
      "winner:  1\n",
      "game 90\n",
      "winner:  1\n",
      "game 91\n",
      "winner:  1\n",
      "game 92\n",
      "winner:  -1\n",
      "game 93\n",
      "winner:  -1\n",
      "game 94\n",
      "winner:  1\n",
      "game 95\n",
      "winner:  -1\n",
      "game 96\n",
      "winner:  -1\n",
      "game 97\n",
      "winner:  -1\n",
      "game 98\n",
      "winner:  -1\n",
      "game 99\n",
      "winner:  1\n",
      "game 100\n",
      "winner:  -1\n",
      "game 101\n",
      "winner:  1\n",
      "game 102\n",
      "winner:  1\n",
      "game 103\n",
      "winner:  1\n",
      "game 104\n",
      "winner:  1\n",
      "game 105\n",
      "winner:  -1\n",
      "game 106\n",
      "winner:  1\n",
      "game 107\n",
      "winner:  -1\n",
      "game 108\n",
      "winner:  1\n",
      "game 109\n",
      "winner:  1\n",
      "game 110\n",
      "winner:  1\n",
      "game 111\n",
      "winner:  1\n",
      "game 112\n",
      "winner:  -1\n",
      "game 113\n",
      "winner:  1\n",
      "game 114\n",
      "winner:  -1\n",
      "game 115\n",
      "winner:  1\n",
      "game 116\n",
      "winner:  1\n",
      "game 117\n",
      "winner:  1\n",
      "game 118\n",
      "winner:  1\n",
      "game 119\n",
      "winner:  -1\n",
      "game 120\n",
      "winner:  -1\n",
      "game 121\n",
      "winner:  -1\n",
      "game 122\n",
      "winner:  1\n",
      "game 123\n",
      "winner:  1\n",
      "game 124\n",
      "winner:  1\n",
      "game 125\n",
      "winner:  -1\n",
      "game 126\n",
      "winner:  1\n",
      "game 127\n",
      "winner:  1\n",
      "game 128\n",
      "winner:  1\n",
      "game 129\n",
      "winner:  1\n",
      "game 130\n",
      "winner:  1\n",
      "game 131\n",
      "winner:  -1\n",
      "game 132\n",
      "winner:  1\n",
      "game 133\n",
      "winner:  -1\n",
      "game 134\n",
      "winner:  -1\n",
      "game 135\n",
      "winner:  1\n",
      "game 136\n",
      "winner:  1\n",
      "game 137\n",
      "winner:  1\n",
      "game 138\n",
      "winner:  1\n",
      "game 139\n",
      "winner:  1\n",
      "game 140\n",
      "winner:  -1\n",
      "game 141\n",
      "winner:  1\n",
      "game 142\n",
      "winner:  -1\n",
      "game 143\n",
      "winner:  -1\n",
      "game 144\n",
      "winner:  1\n",
      "game 145\n",
      "winner:  -1\n",
      "game 146\n",
      "winner:  -1\n",
      "game 147\n",
      "winner:  1\n",
      "game 148\n",
      "winner:  1\n",
      "game 149\n",
      "winner:  -1\n",
      "game 150\n",
      "winner:  -1\n",
      "game 151\n",
      "winner:  -1\n",
      "game 152\n",
      "winner:  0\n",
      "game 153\n",
      "winner:  -1\n",
      "game 154\n",
      "winner:  0\n",
      "game 155\n",
      "winner:  1\n",
      "game 156\n",
      "winner:  1\n",
      "game 157\n",
      "winner:  -1\n",
      "game 158\n",
      "winner:  -1\n",
      "game 159\n",
      "winner:  1\n",
      "game 160\n",
      "winner:  1\n",
      "game 161\n",
      "winner:  1\n",
      "game 162\n",
      "winner:  1\n",
      "game 163\n",
      "winner:  1\n",
      "game 164\n",
      "winner:  -1\n",
      "game 165\n",
      "winner:  -1\n",
      "game 166\n",
      "winner:  -1\n",
      "game 167\n",
      "winner:  1\n",
      "game 168\n",
      "winner:  1\n",
      "game 169\n",
      "winner:  -1\n",
      "game 170\n",
      "winner:  1\n",
      "game 171\n",
      "winner:  1\n",
      "game 172\n",
      "winner:  -1\n",
      "game 173\n",
      "winner:  1\n",
      "game 174\n",
      "winner:  0\n",
      "game 175\n",
      "winner:  -1\n",
      "game 176\n",
      "winner:  1\n",
      "game 177\n",
      "winner:  -1\n",
      "game 178\n",
      "winner:  -1\n",
      "game 179\n",
      "winner:  -1\n",
      "game 180\n",
      "winner:  -1\n",
      "game 181\n",
      "winner:  -1\n",
      "game 182\n",
      "winner:  -1\n",
      "game 183\n",
      "winner:  -1\n",
      "game 184\n",
      "winner:  1\n",
      "game 185\n",
      "winner:  0\n",
      "game 186\n",
      "winner:  1\n",
      "game 187\n",
      "winner:  -1\n",
      "game 188\n",
      "winner:  -1\n",
      "game 189\n",
      "winner:  1\n",
      "game 190\n",
      "winner:  -1\n",
      "game 191\n",
      "winner:  0\n",
      "game 192\n",
      "winner:  -1\n",
      "game 193\n",
      "winner:  -1\n",
      "game 194\n",
      "winner:  -1\n",
      "game 195\n",
      "winner:  1\n",
      "game 196\n",
      "winner:  -1\n",
      "game 197\n",
      "winner:  1\n",
      "game 198\n",
      "winner:  -1\n",
      "game 199\n",
      "winner:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryota\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\ryota\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# ネットワークの初期化\n",
    "policy = p_net()             # 方策NNを作成\n",
    "# policy.load_state_dict(torch.load('bin/policy.pth'))\n",
    "Q_value = v_net()            # 行動価値NNを作成\n",
    "# Q_value.load_state_dict(torch.load('bin/Q_value.pth'))\n",
    "\n",
    "# オプティマイザ、エポック数、バッチサイズ、モンテカルロ・シミュレーションのサンプルサイズを設定\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr = 0.001, weight_decay = 5e-4)\n",
    "Q_value_optimizer = optim.Adam(Q_value.parameters(), lr = 0.001, weight_decay = 5e-4)\n",
    "policy_criterion = nn.CrossEntropyLoss()\n",
    "Q_value_criterion = nn.MSELoss()\n",
    "num_epochs = 10\n",
    "sample_size = 200                               # サンプルサイズ(モンテカルロ・シミュレーションで確率分布を求めるためのサンプル数)  *スケジューリングで変更したい\n",
    "batch_size = int(sample_size/10)                # バッチサイズ\n",
    "\n",
    "# 記録先を指定\n",
    "log_filename = \"log/train_\"+ time.strftime(\"%Y%m%d%H%M\") + \".txt\"\n",
    "# ファイルを初期化\n",
    "with open(log_filename, \"w\") as f:\n",
    "    f.write(time.strftime(\"%Y/%m/%d %H:%M\") + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "time_limit = 18                         # 学習時間の上限(秒)\n",
    "start_time = time.time()                        # ループの開始時刻を取得\n",
    "\n",
    "learn_count = 0\n",
    "while True:\n",
    "    ############### 学習の1ループ開始\n",
    "    # learn_count がきりのいい数になったら、それを表示\n",
    "    if learn_count % 10 == 0:\n",
    "        print(\"===== learn #\",learn_count)\n",
    "    # 対戦の準備\n",
    "    board = Board()             # 盤面を初期化\n",
    "    mctree = MCTree(board)      # モンテカルロ探索木を初期化\n",
    "\n",
    "    # 自己対戦プレイヤーを作成\n",
    "    player = Player(policy, Q_value)      \n",
    "    player.play1batch(mctree, sample_size)  # サンプルサイズ分、自己対戦を行い、学習する\n",
    "    player.fshow(log_filename)\n",
    "    # player.sort_nodes()         # プレイしたノードを訪問回数の降順にソート\n",
    "\n",
    "    # ミニバッチに分割されたデータを作成\n",
    "    dataloader = player.prepare(batch_size) \n",
    "\n",
    "    # ミニバッチ学習\n",
    "    for inputs, policy_targets, reward_targets in dataloader:\n",
    "        # print(input.size())\n",
    "        # print(policy_target.size())\n",
    "        # print(reward_target.size())\n",
    "        # 勾配バッファの初期化\n",
    "        policy_optimizer.zero_grad()\n",
    "        Q_value_optimizer.zero_grad()\n",
    "        # 順伝播\n",
    "        policy_output = policy.forward(inputs)\n",
    "        reward_output = Q_value.forward(inputs)\n",
    "        # 逆伝播\n",
    "        policy.backprop(policy_output, policy_optimizer, policy_criterion, policy_targets)\n",
    "        Q_value.backprop(reward_output, Q_value_optimizer, Q_value_criterion, reward_targets)\n",
    "    \n",
    "    # 学習時間の計測\n",
    "    if time.time() - start_time > time_limit:\n",
    "        break\n",
    "    break\n",
    "    \n",
    "    learn_count += 1\n",
    "    ############### 学習の1ループ終了\n",
    "\n",
    "# モデルの保存\n",
    "torch.save(policy.features.state_dict(), 'bin/policy2.pth')\n",
    "torch.save(policy.state_dict(), 'bin/policy2_sim.pth')\n",
    "torch.save(Q_value.state_dict(), 'bin/Q_value2_sim.pth')\n",
    "torch.save(Q_value.features.state_dict(), 'bin/Q_value2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **学習済みモデルと人間が対戦する**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "先手・後手を選んでください。1:先手(黒×) 2:後手(白◯)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 人とAIの手順を決める\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m先手・後手を選んでください。1:先手(黒×) 2:後手(白◯)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m human \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m human \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      5\u001b[0m     human \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# 黒×\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# 人とAIの手順を決める\n",
    "print(\"先手・後手を選んでください。1:先手(黒×) 2:後手(白◯)\")\n",
    "human = int(input())\n",
    "if human == 1:\n",
    "    human = -1 # 黒×\n",
    "else:\n",
    "    human = 1  # 白◯\n",
    "\n",
    "# モデルの読み込み\n",
    "policy = p_net()\n",
    "policy.load_state_dict(torch.load('bin/policy2_sim.pth'))\n",
    "Q_value = v_net()\n",
    "Q_value.load_state_dict(torch.load('bin/Q_value2_sim.pth'))\n",
    "# 保存先のファイル名\n",
    "filename = \"log/HvsAI_\"+str(time.time())+\".txt\"\n",
    "# 対戦の準備\n",
    "board = Board()             # 盤面を初期化\n",
    "# ログファイルを開く\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(str(time.time())+\"\\n\")\n",
    "    f.write(\"Human: \" + str(human) + \"\\n\")\n",
    "    f.write(\"AI: \" + str(-human) + \"\\n\")\n",
    "    f.write(\"---------<< Hands >>-----------\\n\")\n",
    "\n",
    "# 人とAIの対戦\n",
    "while not board.isEnd():\n",
    "    time.sleep(1.0)                                     # 0.5秒待つ\n",
    "    clear_output()                                      # 画面をクリア\n",
    "    board.show()                                        # 盤面を表示\n",
    "    board.fshow(filename)                            # 盤面をファイルに書き込む\n",
    "    if board.canPlay():\n",
    "        if board.player == human:\n",
    "            print(\"Put coordinate x, y: \", end=\"\")\n",
    "            x, y = map(int, input().split())                # 入力を受け取る\n",
    "            print(\"(\", x, \", \", y,\")\")                      # 入力を表示\n",
    "            if board.canPut(x, y):\n",
    "                board = board.put(x, y)\n",
    "                with open(filename, \"a\") as f:\n",
    "                    f.write(\"\\nHuman: (\" + str(x) + \", \" + str(y) + \")\\n\")\n",
    "            else:\n",
    "                print(\"Can't put\")\n",
    "        else:\n",
    "            mctree = MCTree(board)\n",
    "            player = Player(policy, Q_value)\n",
    "            mctree = player.oneplay(mctree,False)\n",
    "            print(\"AI put \", board.diff(mctree.current.board))\n",
    "            with open(filename, \"a\") as f:\n",
    "                f.write(\"\\nAI: (\" + str(board.diff(mctree.current.board)) + \")\\n\")\n",
    "            board = mctree.current.board\n",
    "    else:\n",
    "        board = board.passPlayer()                      # 石を置けない場合はプレイヤーを交代\n",
    "time.sleep(1.0)\n",
    "clear_output()\n",
    "board.show()\n",
    "board.fshow(filename)\n",
    "board.winner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_param(ax, model, paramname, title = None, **kwargs):\n",
    "    _plot_flag = False\n",
    "    for name, param in model.state_dict().items():\n",
    "        if name == paramname:\n",
    "            ax.hist(param.detach().cpu().numpy().flatten(), **kwargs)\n",
    "            _plot_flag = True\n",
    "            break\n",
    "    ax.set_title(title)\n",
    "    if not _plot_flag:\n",
    "        raise ValueError(\"No such parameter name in the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 4, kernel_size=3, padding=1, padding_mode='replicate'),    # out_channelsは欲しい特徴マップの数\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(4, 16, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 16, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 4, kernel_size=3, padding=1, padding_mode='replicate'),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(4, 1, kernel_size=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias', '4.weight', '4.bias', '6.weight', '6.bias', '8.weight', '8.bias'])\n",
      "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias', '4.weight', '4.bias', '6.weight', '6.bias', '8.weight', '8.bias'])\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "print(model.state_dict().keys())\n",
    "print(policy.features.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryota\\AppData\\Local\\Temp\\ipykernel_25056\\1406790419.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('bin/policy2.pth'))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "title = \"Conv2d_1.bias\"\n",
    "model.load_state_dict(torch.load('bin/policy2.pth'))\n",
    "visualize_param(ax, model, \"4.weight\", title, bins = 100, range = (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def AIvsrandom(policy, Q_value, AI):\n",
    "    board = Board()             # 盤面を初期化\n",
    "    while not board.isEnd():\n",
    "        if board.canPlay():\n",
    "            if board.player == AI:\n",
    "                mctree = MCTree(board)\n",
    "                player = Player(policy, Q_value)\n",
    "                mctree = player.oneplay(mctree,False)\n",
    "                board = mctree.current.board\n",
    "            else:\n",
    "                choices = board.choices()\n",
    "                board = random.choice(list(choices.values()))\n",
    "        else:\n",
    "            board = board.passPlayer()\n",
    "    #board.show()\n",
    "    if board.winner() == AI:\n",
    "        print(\"AI wins!\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"AI loses!\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryota\\AppData\\Local\\Temp\\ipykernel_25056\\341776584.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy.load_state_dict(torch.load('bin/policy2_sim.pth'))\n",
      "C:\\Users\\ryota\\AppData\\Local\\Temp\\ipykernel_25056\\341776584.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Q_value.load_state_dict(torch.load('bin/Q_value2_sim.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "AI loses!\n",
      "AI wins!\n",
      "0.45\n"
     ]
    }
   ],
   "source": [
    "policy = p_net()\n",
    "policy.load_state_dict(torch.load('bin/policy2_sim.pth'))\n",
    "Q_value = v_net()\n",
    "Q_value.load_state_dict(torch.load('bin/Q_value2_sim.pth'))\n",
    "# 対戦の準備\n",
    "board = Board()             # 盤面を初期化\n",
    "win = 0\n",
    "trial = 50\n",
    "for i in range(trial):\n",
    "    board = Board()\n",
    "    win += AIvsrandom(policy, Q_value, -1)\n",
    "    win += AIvsrandom(policy, Q_value, 1)\n",
    "    #print(\"trial\", i)\n",
    "print(win/trial/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chatgptのコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlphaZeroNet(nn.Module):\n",
    "    def __init__(self, board_size, action_space):\n",
    "        super(AlphaZeroNet, self).__init__()\n",
    "        \n",
    "        # Initial convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=17, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Residual blocks (19 as per AlphaZero)\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(256) for _ in range(19)]\n",
    "        )\n",
    "\n",
    "        # Policy head\n",
    "        self.policy_conv = nn.Conv2d(in_channels=256, out_channels=2, kernel_size=1)\n",
    "        self.policy_bn = nn.BatchNorm2d(2)\n",
    "        self.policy_fc = nn.Linear(2 * board_size * board_size, action_space)\n",
    "\n",
    "        # Value head\n",
    "        self.value_conv = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1)\n",
    "        self.value_bn = nn.BatchNorm2d(1)\n",
    "        self.value_fc1 = nn.Linear(board_size * board_size, 256)\n",
    "        self.value_fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Residual blocks\n",
    "        x = self.res_blocks(x)\n",
    "\n",
    "        # Policy head\n",
    "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
    "        p = p.view(p.size(0), -1)\n",
    "        p = F.log_softmax(self.policy_fc(p), dim=1)\n",
    "\n",
    "        # Value head\n",
    "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
    "        v = v.view(v.size(0), -1)\n",
    "        v = F.relu(self.value_fc1(v))\n",
    "        v = torch.tanh(self.value_fc2(v))\n",
    "\n",
    "        return p, v\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        return F.relu(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlphaZeroNetwork(nn.Module):\n",
    "    def __init__(self, board_size, input_planes, residual_blocks=19):\n",
    "        super(AlphaZeroNetwork, self).__init__()\n",
    "        self.board_size = board_size\n",
    "        self.conv1 = nn.Conv2d(input_planes, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            self._build_residual_block(256) for _ in range(residual_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Policy head\n",
    "        self.policy_conv = nn.Conv2d(256, 2, kernel_size=1)\n",
    "        self.policy_bn = nn.BatchNorm2d(2)\n",
    "        self.policy_fc = nn.Linear(2 * board_size * board_size, board_size * board_size * 73)  # 73: チェスの行動次元数\n",
    "        \n",
    "        # Value head\n",
    "        self.value_conv = nn.Conv2d(256, 1, kernel_size=1)\n",
    "        self.value_bn = nn.BatchNorm2d(1)\n",
    "        self.value_fc1 = nn.Linear(board_size * board_size, 256)\n",
    "        self.value_fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "    def _build_residual_block(self, channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        for block in self.residual_blocks:\n",
    "            residual = x\n",
    "            x = block(x)\n",
    "            x += residual  # Skip connection\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # Policy head\n",
    "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
    "        p = self.policy_fc(p.view(p.size(0), -1))\n",
    "        p = F.log_softmax(p, dim=1)\n",
    "        \n",
    "        # Value head\n",
    "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
    "        v = F.relu(self.value_fc1(v.view(v.size(0), -1)))\n",
    "        v = torch.tanh(self.value_fc2(v))\n",
    "        \n",
    "        return p, v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モンテカルロ木探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model, c_puct=1.0, num_simulations=800):\n",
    "        self.model = model\n",
    "        self.c_puct = c_puct\n",
    "        self.num_simulations = num_simulations\n",
    "        self.tree = {}  # 状態 -> ノード情報\n",
    "    \n",
    "    def search(self, state):\n",
    "        for _ in range(self.num_simulations):\n",
    "            self._simulate(state)\n",
    "        return self._get_policy(state)\n",
    "    \n",
    "    def _simulate(self, state):\n",
    "        if state not in self.tree:\n",
    "            # ノードの初期化\n",
    "            p, v = self.model(state)\n",
    "            self.tree[state] = {'N': {}, 'Q': {}, 'P': p, 'V': v}\n",
    "            return v\n",
    "        \n",
    "        node = self.tree[state]\n",
    "        # PUCTスコアを計算し、アクションを選択\n",
    "        total_visits = sum(node['N'].values())\n",
    "        best_action = max(node['N'], key=lambda a: node['Q'][a] + \n",
    "                          self.c_puct * node['P'][a] * math.sqrt(total_visits) / (1 + node['N'][a]))\n",
    "        \n",
    "        # 次の状態をシミュレーション\n",
    "        next_state = self._take_action(state, best_action)\n",
    "        value = self._simulate(next_state)\n",
    "        \n",
    "        # バックプロパゲーション\n",
    "        node['N'][best_action] += 1\n",
    "        node['Q'][best_action] = (node['Q'][best_action] * (node['N'][best_action] - 1) + value) / node['N'][best_action]\n",
    "        return -value\n",
    "    \n",
    "    def _get_policy(self, state):\n",
    "        node = self.tree[state]\n",
    "        visits = np.array([node['N'][a] for a in node['N']])\n",
    "        return visits / sum(visits)\n",
    "    \n",
    "    def _take_action(self, state, action):\n",
    "        # 状態遷移のロジック（具体的にはゲームルールに依存）\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己対戦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play(model, mcts, num_games):\n",
    "    dataset = []\n",
    "    for _ in range(num_games):\n",
    "        game_states = []\n",
    "        current_state = initial_game_state()\n",
    "        while not game_ended(current_state):\n",
    "            policy = mcts.search(current_state)\n",
    "            action = np.random.choice(len(policy), p=policy)\n",
    "            game_states.append((current_state, policy))\n",
    "            current_state = apply_action(current_state, action)\n",
    "        # ゲーム終了後\n",
    "        result = evaluate_game_result(current_state)  # 勝ち:+1, 引き分け:0, 負け:-1\n",
    "        dataset.extend([(state, policy, result) for state, policy in game_states])\n",
    "    return dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
